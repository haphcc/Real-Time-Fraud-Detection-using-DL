{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91718e28"
      },
      "source": [
        "# Task\n",
        "Phân tích tệp `financial_fraud_detection_dataset.csv` để phát hiện gian lận bằng cách tải dữ liệu vào DataFrame của pandas, khám phá cấu trúc, tiền xử lý (bao gồm xử lý đặc trưng phân loại và giá trị thiếu), chia tách dữ liệu thành tập huấn luyện và kiểm tra, chuẩn hóa đặc trưng số, huấn luyện mô hình mạng nơ-ron (ví dụ: `MLPClassifier` từ scikit-learn), và đánh giá hiệu suất mô hình bằng các chỉ số như độ chính xác, độ nhạy, độ chính xác và điểm F1. Cuối cùng, tóm tắt và giải thích chi tiết tất cả các bước đã thực hiện và kết quả đánh giá mô hình bằng tiếng Việt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd1b4cbf"
      },
      "source": [
        "## Tải và Khám phá dữ liệu\n",
        "\n",
        "### Subtask:\n",
        "Tải tệp `financial_fraud_detection_dataset.csv` vào một DataFrame của pandas và khám phá cấu trúc dữ liệu, các cột, loại dữ liệu, và kiểm tra giá trị thiếu. Bước này giúp hiểu rõ hơn về dữ liệu đầu vào.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63b0caec"
      },
      "source": [
        "**Reasoning**:\n",
        "To begin exploring the dataset, I need to import the pandas library, load the CSV file into a DataFrame, display its head, print its information summary, and check for missing values as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d67bb54",
        "outputId": "168478e6-dcde-4a33-d380-ba657bb3e98e"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file into a DataFrame\n",
        "df = pd.read_csv('/content/financial_fraud_detection_dataset.csv')\n",
        "\n",
        "print(\"First 5 rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataFrame Information:\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\nNumber of missing values per column:\")\n",
        "print(df.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the DataFrame:\n",
            "  transaction_id                   timestamp sender_account receiver_account  \\\n",
            "0        T100000  2023-08-22T09:22:43.516168      ACC877572        ACC388389   \n",
            "1        T100001  2023-08-04T01:58:02.606711      ACC895667        ACC944962   \n",
            "2        T100002  2023-05-12T11:39:33.742963      ACC733052        ACC377370   \n",
            "3        T100003  2023-10-10T06:04:43.195112      ACC996865        ACC344098   \n",
            "4        T100004  2023-09-24T08:09:02.700162      ACC584714        ACC497887   \n",
            "\n",
            "    amount transaction_type merchant_category location device_used  is_fraud  \\\n",
            "0   343.78       withdrawal         utilities    Tokyo      mobile     False   \n",
            "1   419.65       withdrawal            online  Toronto         atm     False   \n",
            "2  2773.86          deposit             other   London         pos     False   \n",
            "3  1666.22          deposit            online   Sydney         pos     False   \n",
            "4    24.43         transfer         utilities  Toronto      mobile     False   \n",
            "\n",
            "  fraud_type  time_since_last_transaction  spending_deviation_score  \\\n",
            "0        NaN                          NaN                     -0.21   \n",
            "1        NaN                          NaN                     -0.14   \n",
            "2        NaN                          NaN                     -1.78   \n",
            "3        NaN                          NaN                     -0.60   \n",
            "4        NaN                          NaN                      0.79   \n",
            "\n",
            "   velocity_score  geo_anomaly_score payment_channel       ip_address  \\\n",
            "0               3               0.22            card   13.101.214.112   \n",
            "1               7               0.96             ACH    172.52.47.194   \n",
            "2              20               0.89            card     185.98.35.23   \n",
            "3               6               0.37   wire_transfer    107.136.36.87   \n",
            "4              13               0.27             ACH  108.161.108.255   \n",
            "\n",
            "  device_hash  \n",
            "0    D8536477  \n",
            "1    D2622631  \n",
            "2    D4823498  \n",
            "3    D9961380  \n",
            "4    D7637601  \n",
            "\n",
            "DataFrame Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 251898 entries, 0 to 251897\n",
            "Data columns (total 18 columns):\n",
            " #   Column                       Non-Null Count   Dtype  \n",
            "---  ------                       --------------   -----  \n",
            " 0   transaction_id               251898 non-null  object \n",
            " 1   timestamp                    251898 non-null  object \n",
            " 2   sender_account               251898 non-null  object \n",
            " 3   receiver_account             251898 non-null  object \n",
            " 4   amount                       251898 non-null  float64\n",
            " 5   transaction_type             251898 non-null  object \n",
            " 6   merchant_category            251898 non-null  object \n",
            " 7   location                     251898 non-null  object \n",
            " 8   device_used                  251898 non-null  object \n",
            " 9   is_fraud                     251898 non-null  bool   \n",
            " 10  fraud_type                   1397 non-null    object \n",
            " 11  time_since_last_transaction  32060 non-null   float64\n",
            " 12  spending_deviation_score     251898 non-null  float64\n",
            " 13  velocity_score               251898 non-null  int64  \n",
            " 14  geo_anomaly_score            251898 non-null  float64\n",
            " 15  payment_channel              251898 non-null  object \n",
            " 16  ip_address                   251898 non-null  object \n",
            " 17  device_hash                  251898 non-null  object \n",
            "dtypes: bool(1), float64(4), int64(1), object(12)\n",
            "memory usage: 32.9+ MB\n",
            "\n",
            "Number of missing values per column:\n",
            "transaction_id                      0\n",
            "timestamp                           0\n",
            "sender_account                      0\n",
            "receiver_account                    0\n",
            "amount                              0\n",
            "transaction_type                    0\n",
            "merchant_category                   0\n",
            "location                            0\n",
            "device_used                         0\n",
            "is_fraud                            0\n",
            "fraud_type                     250501\n",
            "time_since_last_transaction    219838\n",
            "spending_deviation_score            0\n",
            "velocity_score                      0\n",
            "geo_anomaly_score                   0\n",
            "payment_channel                     0\n",
            "ip_address                          0\n",
            "device_hash                         0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4b68de7"
      },
      "source": [
        "## Tiền xử lý dữ liệu\n",
        "\n",
        "### Subtask:\n",
        "Chuẩn bị dữ liệu cho việc huấn luyện mô hình. Điều này bao gồm việc xử lý các đặc trưng phân loại (ví dụ: mã hóa one-hot), xử lý các giá trị thiếu nếu có, và chia tách dữ liệu thành tập đặc trưng (X) và biến mục tiêu (y).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd75d74b"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step in data preprocessing is to remove columns that are not necessary for modeling or have too many unique values that could lead to high dimensionality or are directly related to the target variable, as per the instructions. This includes 'transaction_id', 'sender_account', 'receiver_account', 'ip_address', 'device_hash', and 'fraud_type'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11a5d127",
        "outputId": "bc1c101b-fe8e-4b35-89e0-2b34e74184fd"
      },
      "source": [
        "columns_to_drop = ['transaction_id', 'sender_account', 'receiver_account', 'ip_address', 'device_hash', 'fraud_type']\n",
        "df_processed = df.drop(columns=columns_to_drop)\n",
        "\n",
        "print(f\"Columns dropped: {columns_to_drop}\")\n",
        "print(\"Shape of DataFrame after dropping columns:\", df_processed.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns dropped: ['transaction_id', 'sender_account', 'receiver_account', 'ip_address', 'device_hash', 'fraud_type']\n",
            "Shape of DataFrame after dropping columns: (251898, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a71b922"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to convert the 'timestamp' column to datetime objects and then extract new features such as 'hour', 'day_of_week', and 'month' from it, as specified in the instructions. Finally, the original 'timestamp' column will be dropped.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9237b83e",
        "outputId": "12ac4d26-3987-490c-9419-46fc156a9581"
      },
      "source": [
        "df_processed['timestamp'] = pd.to_datetime(df_processed['timestamp'])\n",
        "df_processed['hour'] = df_processed['timestamp'].dt.hour\n",
        "df_processed['day_of_week'] = df_processed['timestamp'].dt.dayofweek\n",
        "df_processed['month'] = df_processed['timestamp'].dt.month\n",
        "df_processed = df_processed.drop(columns=['timestamp'])\n",
        "\n",
        "print(\"DataFrame after extracting time features and dropping 'timestamp' column:\")\n",
        "print(df_processed.head())\n",
        "print(\"Shape of DataFrame after creating new time features:\", df_processed.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame after extracting time features and dropping 'timestamp' column:\n",
            "    amount transaction_type merchant_category location device_used  is_fraud  \\\n",
            "0   343.78       withdrawal         utilities    Tokyo      mobile     False   \n",
            "1   419.65       withdrawal            online  Toronto         atm     False   \n",
            "2  2773.86          deposit             other   London         pos     False   \n",
            "3  1666.22          deposit            online   Sydney         pos     False   \n",
            "4    24.43         transfer         utilities  Toronto      mobile     False   \n",
            "\n",
            "   time_since_last_transaction  spending_deviation_score  velocity_score  \\\n",
            "0                          NaN                     -0.21               3   \n",
            "1                          NaN                     -0.14               7   \n",
            "2                          NaN                     -1.78              20   \n",
            "3                          NaN                     -0.60               6   \n",
            "4                          NaN                      0.79              13   \n",
            "\n",
            "   geo_anomaly_score payment_channel  hour  day_of_week  month  \n",
            "0               0.22            card     9            1      8  \n",
            "1               0.96             ACH     1            4      8  \n",
            "2               0.89            card    11            4      5  \n",
            "3               0.37   wire_transfer     6            1     10  \n",
            "4               0.27             ACH     8            6      9  \n",
            "Shape of DataFrame after creating new time features: (251898, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8bd25f7"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions, I need to handle missing values in the 'time_since_last_transaction' column by filling them with 0. This is based on the assumption that a missing value in this column indicates either the first transaction or a long period since the last one.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bbca769",
        "outputId": "152c79e0-fbc7-494e-8c40-129124368d83"
      },
      "source": [
        "df_processed['time_since_last_transaction'].fillna(0, inplace=True)\n",
        "\n",
        "print(\"Number of missing values in 'time_since_last_transaction' after imputation:\")\n",
        "print(df_processed['time_since_last_transaction'].isnull().sum())\n",
        "print(\"First 5 rows of DataFrame after handling missing values:\")\n",
        "print(df_processed.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing values in 'time_since_last_transaction' after imputation:\n",
            "0\n",
            "First 5 rows of DataFrame after handling missing values:\n",
            "    amount transaction_type merchant_category location device_used  is_fraud  \\\n",
            "0   343.78       withdrawal         utilities    Tokyo      mobile     False   \n",
            "1   419.65       withdrawal            online  Toronto         atm     False   \n",
            "2  2773.86          deposit             other   London         pos     False   \n",
            "3  1666.22          deposit            online   Sydney         pos     False   \n",
            "4    24.43         transfer         utilities  Toronto      mobile     False   \n",
            "\n",
            "   time_since_last_transaction  spending_deviation_score  velocity_score  \\\n",
            "0                          0.0                     -0.21               3   \n",
            "1                          0.0                     -0.14               7   \n",
            "2                          0.0                     -1.78              20   \n",
            "3                          0.0                     -0.60               6   \n",
            "4                          0.0                      0.79              13   \n",
            "\n",
            "   geo_anomaly_score payment_channel  hour  day_of_week  month  \n",
            "0               0.22            card     9            1      8  \n",
            "1               0.96             ACH     1            4      8  \n",
            "2               0.89            card    11            4      5  \n",
            "3               0.37   wire_transfer     6            1     10  \n",
            "4               0.27             ACH     8            6      9  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3920246974.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_processed['time_since_last_transaction'].fillna(0, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc024ef8"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step produced a `FutureWarning` due to the use of `inplace=True` with chained assignment. To fix this, I will modify the line to directly assign the result of `fillna(0)` back to the column, which is the recommended practice for avoiding such warnings and ensuring the operation is performed on the DataFrame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "671cce03",
        "outputId": "3aa56434-cb32-439d-cb45-442b1e6c40be"
      },
      "source": [
        "df_processed['time_since_last_transaction'] = df_processed['time_since_last_transaction'].fillna(0)\n",
        "\n",
        "print(\"Number of missing values in 'time_since_last_transaction' after imputation:\")\n",
        "print(df_processed['time_since_last_transaction'].isnull().sum())\n",
        "print(\"First 5 rows of DataFrame after handling missing values:\")\n",
        "print(df_processed.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing values in 'time_since_last_transaction' after imputation:\n",
            "0\n",
            "First 5 rows of DataFrame after handling missing values:\n",
            "    amount transaction_type merchant_category location device_used  is_fraud  \\\n",
            "0   343.78       withdrawal         utilities    Tokyo      mobile     False   \n",
            "1   419.65       withdrawal            online  Toronto         atm     False   \n",
            "2  2773.86          deposit             other   London         pos     False   \n",
            "3  1666.22          deposit            online   Sydney         pos     False   \n",
            "4    24.43         transfer         utilities  Toronto      mobile     False   \n",
            "\n",
            "   time_since_last_transaction  spending_deviation_score  velocity_score  \\\n",
            "0                          0.0                     -0.21               3   \n",
            "1                          0.0                     -0.14               7   \n",
            "2                          0.0                     -1.78              20   \n",
            "3                          0.0                     -0.60               6   \n",
            "4                          0.0                      0.79              13   \n",
            "\n",
            "   geo_anomaly_score payment_channel  hour  day_of_week  month  \n",
            "0               0.22            card     9            1      8  \n",
            "1               0.96             ACH     1            4      8  \n",
            "2               0.89            card    11            4      5  \n",
            "3               0.37   wire_transfer     6            1     10  \n",
            "4               0.27             ACH     8            6      9  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db08181e"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that missing values are handled, I will define the target variable `y` as 'is_fraud' and the feature set `X` as all other columns from `df_processed`. This step is crucial for preparing the data for model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7928c5c4",
        "outputId": "fc459ff4-5370-4d9b-d572-9fd638351f36"
      },
      "source": [
        "y = df_processed['is_fraud']\n",
        "X = df_processed.drop(columns=['is_fraud'])\n",
        "\n",
        "print(\"Shape of X (features):\", X.shape)\n",
        "print(\"Shape of y (target):\", y.shape)\n",
        "print(\"First 5 rows of y:\")\n",
        "print(y.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X (features): (251898, 13)\n",
            "Shape of y (target): (251898,)\n",
            "First 5 rows of y:\n",
            "0    False\n",
            "1    False\n",
            "2    False\n",
            "3    False\n",
            "4    False\n",
            "Name: is_fraud, dtype: bool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3532476"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step in data preprocessing is to apply one-hot encoding to all categorical columns (columns with 'object' dtype) in the feature set `X` using `pd.get_dummies()`, as specified in the instructions. This will convert the categorical features into a numerical format suitable for machine learning models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db49533f",
        "outputId": "aef649d4-a1da-49fe-b696-a50c341f3e5f"
      },
      "source": [
        "X_processed = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "print(\"Shape of X after one-hot encoding:\", X_processed.shape)\n",
        "print(\"First 5 rows of X after one-hot encoding:\")\n",
        "print(X_processed.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after one-hot encoding: (251898, 31)\n",
            "First 5 rows of X after one-hot encoding:\n",
            "    amount  time_since_last_transaction  spending_deviation_score  \\\n",
            "0   343.78                          0.0                     -0.21   \n",
            "1   419.65                          0.0                     -0.14   \n",
            "2  2773.86                          0.0                     -1.78   \n",
            "3  1666.22                          0.0                     -0.60   \n",
            "4    24.43                          0.0                      0.79   \n",
            "\n",
            "   velocity_score  geo_anomaly_score  hour  day_of_week  month  \\\n",
            "0               3               0.22     9            1      8   \n",
            "1               7               0.96     1            4      8   \n",
            "2              20               0.89    11            4      5   \n",
            "3               6               0.37     6            1     10   \n",
            "4              13               0.27     8            6      9   \n",
            "\n",
            "   transaction_type_payment  transaction_type_transfer  ...  \\\n",
            "0                     False                      False  ...   \n",
            "1                     False                      False  ...   \n",
            "2                     False                      False  ...   \n",
            "3                     False                      False  ...   \n",
            "4                     False                       True  ...   \n",
            "\n",
            "   location_Singapore  location_Sydney  location_Tokyo  location_Toronto  \\\n",
            "0               False            False            True             False   \n",
            "1               False            False           False              True   \n",
            "2               False            False           False             False   \n",
            "3               False             True           False             False   \n",
            "4               False            False           False              True   \n",
            "\n",
            "   device_used_mobile  device_used_pos  device_used_web  payment_channel_UPI  \\\n",
            "0                True            False            False                False   \n",
            "1               False            False            False                False   \n",
            "2               False             True            False                False   \n",
            "3               False             True            False                False   \n",
            "4                True            False            False                False   \n",
            "\n",
            "   payment_channel_card  payment_channel_wire_transfer  \n",
            "0                  True                          False  \n",
            "1                 False                          False  \n",
            "2                  True                          False  \n",
            "3                 False                           True  \n",
            "4                 False                          False  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "671973f5"
      },
      "source": [
        "## Chia tách và Chuẩn hóa dữ liệu\n",
        "\n",
        "### Subtask:\n",
        "Chia dữ liệu đã tiền xử lý thành các tập huấn luyện và kiểm tra. Chuẩn hóa (scaling) các đặc trưng số để tối ưu hóa hiệu suất của mô hình và đảm bảo tất cả các đặc trưng đóng góp tương đương vào quá trình huấn luyện.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78f55fe6"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to split the preprocessed data into training and testing sets using `train_test_split` as instructed, which is crucial for evaluating the model's performance on unseen data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc633a1c",
        "outputId": "767fa11d-7ac7-4398-f7ca-76870d591a49"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (201518, 31)\n",
            "Shape of X_test: (50380, 31)\n",
            "Shape of y_train: (201518,)\n",
            "Shape of y_test: (50380,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e25d3457"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the data is split into training and testing sets, I need to standardize the numerical features to ensure all features contribute equally to the model training, as per the instructions. This involves identifying numerical columns, initializing a `StandardScaler`, fitting it on the numerical columns of the training set, and then transforming both the training and test sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7db41910",
        "outputId": "0c4bf482-2e2f-43ba-c677-f84571c42093"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Identify numerical columns (those not created by one-hot encoding)\n",
        "# By checking the dtypes, original numerical columns are float64 or int64 before one-hot encoding.\n",
        "# After one-hot encoding, they retain their types, while new columns are boolean.\n",
        "# We can assume numerical features are those that are not boolean.\n",
        "numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the numerical columns of the training data\n",
        "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
        "\n",
        "# Transform the numerical columns of both training and test data\n",
        "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
        "\n",
        "print(\"First 5 rows of X_train after scaling numerical features:\")\n",
        "print(X_train.head())\n",
        "print(\"\\nFirst 5 rows of X_test after scaling numerical features:\")\n",
        "print(X_test.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of X_train after scaling numerical features:\n",
            "          amount  time_since_last_transaction  spending_deviation_score  \\\n",
            "27542  -0.372807                     0.000166                 -1.694139   \n",
            "171173  0.242596                     0.000166                 -0.102541   \n",
            "66303   0.163053                     0.000166                  0.047609   \n",
            "10474  -0.747608                     0.000166                 -0.983426   \n",
            "31862  -0.329426                     0.000166                 -1.794240   \n",
            "\n",
            "        velocity_score  geo_anomaly_score  hour  day_of_week  month  \\\n",
            "27542        -0.780948           1.006859    11            0      1   \n",
            "171173        0.779988           1.180182     5            0      5   \n",
            "66303         0.953425          -1.419673    18            0      5   \n",
            "10474         0.433113          -0.102413    13            5     11   \n",
            "31862        -0.607511           0.278899     7            5      6   \n",
            "\n",
            "        transaction_type_payment  transaction_type_transfer  ...  \\\n",
            "27542                       True                      False  ...   \n",
            "171173                     False                      False  ...   \n",
            "66303                      False                      False  ...   \n",
            "10474                      False                       True  ...   \n",
            "31862                      False                      False  ...   \n",
            "\n",
            "        location_Singapore  location_Sydney  location_Tokyo  location_Toronto  \\\n",
            "27542                False             True           False             False   \n",
            "171173                True            False           False             False   \n",
            "66303                False            False           False             False   \n",
            "10474                 True            False           False             False   \n",
            "31862                False            False           False             False   \n",
            "\n",
            "        device_used_mobile  device_used_pos  device_used_web  \\\n",
            "27542                False            False            False   \n",
            "171173               False             True            False   \n",
            "66303                False            False            False   \n",
            "10474                 True            False            False   \n",
            "31862                False             True            False   \n",
            "\n",
            "        payment_channel_UPI  payment_channel_card  \\\n",
            "27542                 False                  True   \n",
            "171173                False                 False   \n",
            "66303                  True                 False   \n",
            "10474                 False                 False   \n",
            "31862                  True                 False   \n",
            "\n",
            "        payment_channel_wire_transfer  \n",
            "27542                           False  \n",
            "171173                          False  \n",
            "66303                           False  \n",
            "10474                            True  \n",
            "31862                           False  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "\n",
            "First 5 rows of X_test after scaling numerical features:\n",
            "          amount  time_since_last_transaction  spending_deviation_score  \\\n",
            "228315 -0.722021                    -2.940180                 -0.462903   \n",
            "26913  -0.463774                     0.000166                  0.608172   \n",
            "38497   1.758977                     0.000166                 -0.382823   \n",
            "82756   1.075541                     0.619510                  1.398966   \n",
            "213140  2.777380                    -1.647786                  0.137700   \n",
            "\n",
            "        velocity_score  geo_anomaly_score  hour  day_of_week  month  \\\n",
            "228315        1.647175          -1.003696    21            1      3   \n",
            "26913        -0.954386          -0.241072     7            4      5   \n",
            "38497        -1.648135          -1.281014     3            1      3   \n",
            "82756         1.473737           0.764206    15            5      8   \n",
            "213140        1.473737           0.244235    10            5      1   \n",
            "\n",
            "        transaction_type_payment  transaction_type_transfer  ...  \\\n",
            "228315                      True                      False  ...   \n",
            "26913                       True                      False  ...   \n",
            "38497                      False                      False  ...   \n",
            "82756                      False                      False  ...   \n",
            "213140                     False                      False  ...   \n",
            "\n",
            "        location_Singapore  location_Sydney  location_Tokyo  location_Toronto  \\\n",
            "228315               False            False           False             False   \n",
            "26913                False            False           False             False   \n",
            "38497                False            False           False             False   \n",
            "82756                False            False           False              True   \n",
            "213140               False             True           False             False   \n",
            "\n",
            "        device_used_mobile  device_used_pos  device_used_web  \\\n",
            "228315               False             True            False   \n",
            "26913                False             True            False   \n",
            "38497                False             True            False   \n",
            "82756                False            False            False   \n",
            "213140               False             True            False   \n",
            "\n",
            "        payment_channel_UPI  payment_channel_card  \\\n",
            "228315                False                  True   \n",
            "26913                 False                 False   \n",
            "38497                 False                 False   \n",
            "82756                 False                 False   \n",
            "213140                False                 False   \n",
            "\n",
            "        payment_channel_wire_transfer  \n",
            "228315                          False  \n",
            "26913                            True  \n",
            "38497                            True  \n",
            "82756                           False  \n",
            "213140                           True  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72934f3e"
      },
      "source": [
        "## Huấn luyện Mô hình Mạng Nơ-ron\n",
        "\n",
        "### Subtask:\n",
        "Xây dựng và huấn luyện một mô hình mạng nơ-ron đơn giản (ví dụ: `MLPClassifier` từ thư viện scikit-learn, đại diện cho một mô hình mạng nơ-ron cho dữ liệu bảng) trên tập dữ liệu huấn luyện. Giải thích rằng mô hình này được chọn vì \"HTGNN\" không phải là một mô hình tiêu chuẩn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5983be6b"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to import `MLPClassifier`, initialize it with specified parameters, and then fit it on the `X_train` and `y_train` data as per the instructions. Finally, I should print a confirmation message.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2c953c8",
        "outputId": "e26c07a4-7206-49fa-ac2c-78b57b1e01e2"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Initialize the MLPClassifier model\n",
        "# Using hidden_layer_sizes=(100,), max_iter=100, and random_state=42 as initial parameters\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=100, random_state=42, verbose=True)\n",
        "\n",
        "# Train the model\n",
        "print(\"Training MLPClassifier...\")\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "print(\"MLPClassifier training complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MLPClassifier...\n",
            "Iteration 1, loss = 0.03537676\n",
            "Iteration 2, loss = 0.03101891\n",
            "Iteration 3, loss = 0.03009368\n",
            "Iteration 4, loss = 0.02959512\n",
            "Iteration 5, loss = 0.02919465\n",
            "Iteration 6, loss = 0.02863791\n",
            "Iteration 7, loss = 0.02818207\n",
            "Iteration 8, loss = 0.02789934\n",
            "Iteration 9, loss = 0.02746690\n",
            "Iteration 10, loss = 0.02708533\n",
            "Iteration 11, loss = 0.02692524\n",
            "Iteration 12, loss = 0.02659947\n",
            "Iteration 13, loss = 0.02646306\n",
            "Iteration 14, loss = 0.02616153\n",
            "Iteration 15, loss = 0.02599342\n",
            "Iteration 16, loss = 0.02587097\n",
            "Iteration 17, loss = 0.02567453\n",
            "Iteration 18, loss = 0.02556704\n",
            "Iteration 19, loss = 0.02541843\n",
            "Iteration 20, loss = 0.02517416\n",
            "Iteration 21, loss = 0.02503104\n",
            "Iteration 22, loss = 0.02491958\n",
            "Iteration 23, loss = 0.02497839\n",
            "Iteration 24, loss = 0.02471233\n",
            "Iteration 25, loss = 0.02461736\n",
            "Iteration 26, loss = 0.02435466\n",
            "Iteration 27, loss = 0.02441557\n",
            "Iteration 28, loss = 0.02423349\n",
            "Iteration 29, loss = 0.02416855\n",
            "Iteration 30, loss = 0.02400673\n",
            "Iteration 31, loss = 0.02398387\n",
            "Iteration 32, loss = 0.02380882\n",
            "Iteration 33, loss = 0.02371250\n",
            "Iteration 34, loss = 0.02359591\n",
            "Iteration 35, loss = 0.02367734\n",
            "Iteration 36, loss = 0.02337787\n",
            "Iteration 37, loss = 0.02334968\n",
            "Iteration 38, loss = 0.02327149\n",
            "Iteration 39, loss = 0.02330166\n",
            "Iteration 40, loss = 0.02302491\n",
            "Iteration 41, loss = 0.02297902\n",
            "Iteration 42, loss = 0.02297696\n",
            "Iteration 43, loss = 0.02287636\n",
            "Iteration 44, loss = 0.02274771\n",
            "Iteration 45, loss = 0.02262094\n",
            "Iteration 46, loss = 0.02257312\n",
            "Iteration 47, loss = 0.02256942\n",
            "Iteration 48, loss = 0.02232672\n",
            "Iteration 49, loss = 0.02236090\n",
            "Iteration 50, loss = 0.02242563\n",
            "Iteration 51, loss = 0.02238481\n",
            "Iteration 52, loss = 0.02215784\n",
            "Iteration 53, loss = 0.02218273\n",
            "Iteration 54, loss = 0.02211627\n",
            "Iteration 55, loss = 0.02206693\n",
            "Iteration 56, loss = 0.02206038\n",
            "Iteration 57, loss = 0.02183734\n",
            "Iteration 58, loss = 0.02181150\n",
            "Iteration 59, loss = 0.02176129\n",
            "Iteration 60, loss = 0.02173320\n",
            "Iteration 61, loss = 0.02148695\n",
            "Iteration 62, loss = 0.02164606\n",
            "Iteration 63, loss = 0.02147839\n",
            "Iteration 64, loss = 0.02148598\n",
            "Iteration 65, loss = 0.02144981\n",
            "Iteration 66, loss = 0.02133182\n",
            "Iteration 67, loss = 0.02115101\n",
            "Iteration 68, loss = 0.02129964\n",
            "Iteration 69, loss = 0.02109695\n",
            "Iteration 70, loss = 0.02125369\n",
            "Iteration 71, loss = 0.02096577\n",
            "Iteration 72, loss = 0.02109311\n",
            "Iteration 73, loss = 0.02103199\n",
            "Iteration 74, loss = 0.02082317\n",
            "Iteration 75, loss = 0.02110224\n",
            "Iteration 76, loss = 0.02078951\n",
            "Iteration 77, loss = 0.02082580\n",
            "Iteration 78, loss = 0.02088760\n",
            "Iteration 79, loss = 0.02075396\n",
            "Iteration 80, loss = 0.02060869\n",
            "Iteration 81, loss = 0.02066402\n",
            "Iteration 82, loss = 0.02071241\n",
            "Iteration 83, loss = 0.02056889\n",
            "Iteration 84, loss = 0.02046917\n",
            "Iteration 85, loss = 0.02048618\n",
            "Iteration 86, loss = 0.02043764\n",
            "Iteration 87, loss = 0.02042034\n",
            "Iteration 88, loss = 0.02038662\n",
            "Iteration 89, loss = 0.02032408\n",
            "Iteration 90, loss = 0.02022846\n",
            "Iteration 91, loss = 0.02019512\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "MLPClassifier training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e886117"
      },
      "source": [
        "## Đánh giá hiệu suất mô hình\n",
        "\n",
        "### Subtask:\n",
        "Đánh giá hiệu suất của mô hình mạng nơ-ron đã huấn luyện trên tập kiểm tra (`X_test`, `y_test`) bằng cách tính toán các chỉ số như độ chính xác (accuracy), độ nhạy (recall), độ chính xác (precision) và điểm F1. Các chỉ số này sẽ giúp đánh giá mức độ hiệu quả của mô hình trong việc phát hiện gian lận.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b64ca9e5"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the model has been trained, I need to predict on the test set and calculate evaluation metrics such as accuracy, precision, recall, and F1-score to assess its performance as specified in the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a68c97fe",
        "outputId": "842ae705-d508-4db2-86a6-cd9ce4938cb1"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Evaluation Metrics:\\n\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Evaluation Metrics:\n",
            "\n",
            "Accuracy: 0.9933\n",
            "Precision: 0.0444\n",
            "Recall: 0.0068\n",
            "F1-Score: 0.0117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e14ce29"
      },
      "source": [
        "## Hoàn thành tác vụ\n",
        "\n",
        "### Subtask:\n",
        "Tóm tắt kết quả đánh giá mô hình và cung cấp giải thích chi tiết về các bước đã thực hiện, tất cả bằng tiếng Việt. Giải thích rằng một mạng nơ-ron tổng quát đã được sử dụng và nếu \"HTGNN\" có ý nghĩa cụ thể liên quan đến đồ thị hoặc kiến trúc đặc biệt, cần cung cấp thêm thông tin để tùy chỉnh mô hình.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae027b8d"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "Dưới đây là tóm tắt quá trình giải quyết nhiệm vụ:\n",
        "\n",
        "### Q&A\n",
        "Nhiệm vụ yêu cầu tóm tắt kết quả đánh giá mô hình và cung cấp giải thích chi tiết về các bước đã thực hiện. Một mạng nơ-ron tổng quát đã được sử dụng (MLPClassifier). Nếu \"HTGNN\" có ý nghĩa cụ thể liên quan đến đồ thị hoặc kiến trúc đặc biệt, cần cung cấp thêm thông tin để tùy chỉnh mô hình. Trong trường hợp này, \"HTGNN\" không được định nghĩa rõ ràng, nên mô hình MLPClassifier đã được chọn làm đại diện cho mạng nơ-ron tổng quát.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Dữ liệu ban đầu**: Tập dữ liệu `financial_fraud_detection_dataset.csv` chứa 251.898 giao dịch với 18 cột.\n",
        "    *   Các cột `fraud_type` và `time_since_last_transaction` có số lượng giá trị thiếu đáng kể (250.501 và 219.838 tương ứng).\n",
        "*   **Tiền xử lý dữ liệu**:\n",
        "    *   Các cột không cần thiết (`transaction_id`, `sender_account`, `receiver_account`, `ip_address`, `device_hash`, `fraud_type`) đã được loại bỏ.\n",
        "    *   Đã trích xuất các đặc trưng thời gian (`hour`, `day_of_week`, `month`) từ cột `timestamp`.\n",
        "    *   Các giá trị thiếu trong `time_since_last_transaction` đã được điền bằng 0.\n",
        "    *   Dữ liệu được chia thành tập đặc trưng (X) và biến mục tiêu (y - `is_fraud`).\n",
        "    *   Các đặc trưng phân loại đã được mã hóa one-hot, dẫn đến tập đặc trưng `X_processed` có kích thước (251898, 31).\n",
        "*   **Chia tách và Chuẩn hóa dữ liệu**:\n",
        "    *   Dữ liệu được chia thành tập huấn luyện (80%, 201.518 mẫu) và tập kiểm tra (20%, 50.380 mẫu) với `random_state=42`.\n",
        "    *   Các đặc trưng số đã được chuẩn hóa bằng `StandardScaler`.\n",
        "*   **Huấn luyện mô hình**: Một mô hình `MLPClassifier` với `hidden_layer_sizes=(100,)` và `max_iter=100` đã được huấn luyện thành công trên tập dữ liệu huấn luyện.\n",
        "*   **Đánh giá hiệu suất mô hình**: Trên tập kiểm tra, mô hình đạt được các chỉ số sau:\n",
        "    *   Độ chính xác (Accuracy): 0.9933\n",
        "    *   Độ chính xác (Precision) cho lớp gian lận: 0.0444\n",
        "    *   Độ nhạy (Recall) cho lớp gian lận: 0.0068\n",
        "    *   Điểm F1 (F1-Score) cho lớp gian lận: 0.0117\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Mặc dù mô hình đạt độ chính xác tổng thể cao (99.33%), các chỉ số `Precision`, `Recall` và `F1-Score` cực kỳ thấp cho việc phát hiện giao dịch gian lận. Điều này cho thấy mô hình không hiệu quả trong việc xác định các trường hợp gian lận thực tế, có thể là do sự mất cân bằng dữ liệu nghiêm trọng (số lượng giao dịch gian lận rất ít).\n",
        "*   Để cải thiện hiệu suất phát hiện gian lận, cần áp dụng các kỹ thuật xử lý dữ liệu mất cân bằng (ví dụ: Oversampling, Undersampling, SMOTE) hoặc sử dụng các thuật toán nhận biết mất cân bằng dữ liệu, và tinh chỉnh các tham số của `MLPClassifier` hoặc thử các mô hình khác phù hợp hơn với dữ liệu mất cân bằng.\n"
      ]
    }
  ]
}